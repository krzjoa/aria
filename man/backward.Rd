% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/backward.R
\name{backward}
\alias{backward}
\title{This function traverses the graph back and computes all the gradients where it's needed}
\usage{
backward(ops, gradient)
}
\arguments{
\item{ops}{tensor}

\item{gradient}{TODO: potentially hard to handle: dimshuffles etc.
TODO: transform into S4 method

TODO: unify names ops vs ops_ptr
TODO: flatten graph(?)

We need to propagate information, if gradient is required or not}
}
\description{
https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95
https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf
http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/readings/L06%20Automatic%20Differentiation.pdf
https://j-towns.github.io/2017/06/12/A-new-trick.html
See: https://pytorch.org/docs/stable/autograd.html
https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_vjps.py
}
\examples{
library(dlr)
ctx <- get_context()
register_ops(ctx, cars)
register_ops(ctx, data.frame)
x <- cpu_tensor(5, dims = 1)
y <- (x ** 3) / 2
bkw_fun <- get_inputs(y)[[1]]
get_paired_object(bkw_fun)
all.ops <- get_all_ops_ptr(ctx)
backward(y, 1)
TODO: sposób iteacji - obecnie idziemy jak najbardziej wgłąb
TODO: powstają jakieś niezamiezone kopie tensorów - gdzie i kiedy?
For simplicity, suppose we have only
one sequence of operations
if (!inherits(ops, "cpu_tensor"))
  stop("Error!")
For every input to the function
1. Find all the argumnets
2. Get matching function
}
